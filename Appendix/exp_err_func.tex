\chapter{Expectation value of the error function}
\label{app:exp_err_func}

The error function subjected to the minimisation procedure does not correspond to the usual $\chi^2$ used to quantify the quality of the fit. Indeed, the residue is computed w.r.t. the fluctuated data points and thus mean value of the error function converges to $2$ instead of $1$. In the following, I will give a proof for that.%

First, let us introduce a set of $N_{\T{dat}}$ data points distributed according to a multivariate Gaussian distribution $\mathcal{N}(\mb{d},\mb{V})$, with mean values $\mb{d} = (d_1, \dots, d_{N_{\T{dat}}})$ and covariance matrix $\mb{V}$. As explained in \secref{sec:optimisation}, artificial data points $\mb{d}^{(k)}$ are generated out of the original data set through the Monte Carlo sampling method and then compared with the predictions of the $k$-th replica $\mb{t}^{(k)}$ by means of the error function, whose definition is reported here for convenience
%
\begin{equation}
    E^{(k)} = \frac{1}{N_{\T{dat}}} \sum_{i,j}^{N_{\T{dat}}} \left( \mb{t}^{(k)} - \mb{d}^{(k)} \right) (\T{cov})^{-1}_{ij} \left( \mb{t}^{(k)} - \mb{d}^{(k)} \right) \,.
  \label{eq:app:E}
\end{equation}
%
The key point to bear in mind is that both the artificial points $\mb{d^{(k)}}$ and the predictions related to the $k$-th replica $\mb{t}^{(k)}$ are distributed according to the same distribution of the original data set, that is $\mb{d^{(k)}}, \, \mb{t}^{(k)} \sim \mathcal{N}(\mb{d},\mb{V})$. As explicitly shown in \secref{sec:gen_MC}, artificial points are generated as follows
%
\begin{equation}
    \mb{d}^{(k)} = \mb{d} + \mb{L} \cdot \mb{z}^{(k)} \,,
    \label{eq:app:MC}
\end{equation}
%
where $\mb{L}$ is the lower triangular matrix from the Cholesky decomposition of the covariance matrix $\mb{V}$ (see \secref{sec:gen_MC} and App.\ref{app:lin_sys}) whereas $\mb{z} = (z_1, \, z_2, \dots,\, z_{N_{\T{dat}}})$ is distributed according to $\mb{z} \sim \mathcal{N}(\mb{0},\; I_{N_{\T{dat}}})$. By substituting Eq.~\eqref{eq:app:MC} in Eq.~\eqref{eq:app:E}, the error function can be recast as
%
\begin{equation}
    \begin{split}
        E \hspace{2mm} & = \hspace{2mm} \frac{1}{N_{\T{dat}}} \left( \mb{t}^{(k)} - \mb{d}\right)^T \, \mb{V}^{-1} \, \left( \mb{t}^{(k)} - \mb{d} \right) + \frac{1}{N_{\T{dat}}} \left( \mb{L} \cdot \mb{z}^{(k)} \right)^T \, \mb{V}^{-1} \left(  \mb{L} \cdot \mb{z}^{(k)} \right) \\
        & \hspace{5mm} - \frac{2}{N_{\T{dat}}} \left( \mb{t}^{(k)} - \mb{d} \right)^T \, \left(\mb{L} \mb{L}^{T} \right)^{-1} \, \left(\mb{L} \cdot \mb{z}^{(k)} \right) \\
        & = \left(\chi^2\right) + \frac{\left| \mb{z}^{(k)} \right|^2}{N_{\T{dat}}} - \frac{2}{N_{\T{dat}}} \left(\mb{w}^{(k)} \right)^T  \, \mb{z}^{(k)} \;,
    \end{split}
    \label{eq:app:E_split}
\end{equation}
%
where $\mb{w}^{(k)} = \mb{L}^{-1} \cdot \left( \mb{t}^{(k)} - \mb{d} \right)$. In moving from the second to the third equality of Eq.~\eqref{eq:app:E_split}, I made use of the Cholesky decomposition of the covariance matrix, that is $\mb{V} = \mb{L} \mb{L}^{T}$. The equation above shows that the error function is the sum of three different contributions. The first term is the $\chi^2$ of the prediction evaluated w.r.t. the central value $\mb{d}$. The second one is the norm of the random vector $\mb{z}^{(k)}$ normalized to the number of data points, while the last one, more involved, is the product of two random variables. By linearity, the expectation value of the error function over all the replicas is the sum of the expectation values of each term appearing in the last line of Eq.~\eqref{eq:app:E_split}:
%%
\begin{equation}
    \left< E^{(k)} \right>_{\T{rep}} = \left<(\chi^2)^{(k)}\right>_{\T{rep}} + \frac{1}{N_{\T{dat}}} \left< \left| \mb{z}^{(k)} \right|^2 \right>_{\T{rep}} - \frac{2}{N_{\T{dat}}} \left<\left(\mb{w}^{(k)}\right)^T  \, \mb{z}^{(k)}\right>_{\T{rep}} \;.
    \label{eq:app:exp_E_split}
\end{equation}
%%
The first term is 1 since it corresponds to the expectation value of a variable distributed according to a $\chi^2$ distribution. The second one is the sum of the expectation values of the squared elements contained in the $N_{\T{dat}}$ dimensional vector $\mb{z}^{(k)}$. Given that $\mb{z}^{(k)} \sim \mathcal{N}(\mb{0},\, I_{N_{dat}})$, then $\left< \left| \mb{z}^{(k)} \right|^2 \right>_{\T{rep}} = N_{\T{dat}}$ and thus the second term is again 1. All we have left to do is to prove that the last term is zero. First, I am going to work out what distribution the vector $\mb{w}^{(k)}$ belongs to. For the sake of simplicity, I introduce the vector $\mb{y}^{(k)} = \mb{t}^{(k)} - \mb{d}$, and I can write
%%
\begin{equation}
    \mb{w}^{(k)} = \mb{L}^{-1} \cdot \mb{y}^{(k)} \;.
\end{equation}
%%
It must be observed that $\mb{y}^{(k)}$ follows the same distribution of $\mb{t}^{(k)}$, whose mean value is now shifted by a factor of $\mb{d}$. Thus, the probability density function of $\mb{y}^{(k)}$ is a multivariate Gaussian given by
%%
\begin{equation}
    f_{\mb{y}} = (2\pi)^{-N_{\T{dat}}/2} \; \left| \mb{V} \right|^{-1/2} \; \exp{-\frac{1}{2}{\mb{y}}^{(k)T} \; \mb{V}^{-1} \; \mb{y}^{(k)}},
\end{equation}
%%
where $\left| \mb{V} \right|$ denotes the determinant of the covariance matrix $\mb{V}$. Exploiting the Cholesky decomposition, the determinant can be written as 
%%
\begin{equation}
    \left| \mb{V} \right| = \left| \mb{L} \, \mb{L}^T \right| = \left| \mb{L} \right|^2 \;.
\end{equation}
%%
From the properties of probability functions, one can compute the transformed density $f_w$ as follows (see \textit{e.g.} Ref.~\cite{Bohm:2005bu})
%%
\begin{equation}
    f_{\mb{w}} = f_{\mb{y}} \left| \dv{\mb{y}}{\mb{w}} \right| = \left|\mb{L} \right| \, f_{\mb{y}} = (2\pi)^{-N_{\T{dat}}/2} \; \exp{-\frac{1}{2}\mb{w}^{(k)T} \; \mb{w}^{(k)}} \; .
\end{equation}
%%
Thus, the vector $\mb{w}$ is also distributed according to a multivariate Gaussian distribution $\mathcal{N} (\mb{0}, I_{N_{\T{dat}}})$. The product of random variables appearing in the last element of Eq.~\eqref{eq:app:exp_E_split} is then distributed according to the convolution of two Gaussian distributions both centred in zero. Since the mean value of convoluted Gaussian distributions is just the sum of the parents mean values, the expectation values of such a product is zero.
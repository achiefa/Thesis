\documentclass[../main.tex]{subfiles}

\begin{document}
\newcommand{\mb}[1]{\mathbf{#1}}

\chapter{Linear Systems}
In linear algebra, the Cholesky decomposition (or factorization) decomposes an hermitian, positive-definite matrix into a product of a lower triangular matrix and its conjugate. The formulation of the Cholesky decomposition requires some basic concepts in linear systems, that we shall briefly review.

\section{Triangular Systems}
The problem of finding a solution for a linear system $\mb{V} \cdot \mb{x} = \mb{b}$ appears frequently when dealing with numerical computation. Gaussian elimination provides an efficient way to solve such systems by converting the  original system into an equivalent triangular one. Thus, it is worth spending some words to outline basic properties and relations about triangular systems.\par
Let us consider a linear system written in matrix notation
\\
\begin{equation}
    \mb{L} \cdot \mb{x} = \mb{b} \hspace{10mm} \textrm{or} \hspace{10mm} \mb{U} \cdot \mb{x} = \mb{b} \; ,
\end{equation}
\\
where $\mb{L}$ and $\mb{U}$ are lower and upper triangular matrices respectively. These systems can be solved in $\mb{x}$ by means of an iterative process called \textit{forward substitution} for lower triangular matrices and \textit{backward substitution} for upper triangular matrices. 


\subsection*{Forward and Backward Substitution}
Given a lower triangular matrix $\mb{L} \in \mathbb{R}^{n \times n}$, the matrix equation $\mb{L} \cdot \mb{x} = \mb{b}$ can be written as a system of n linear equations
\\
\begin{equation*}
\begin{matrix}
  \ell_{1,1} x_1 &   &                &   &        &   &                & = &    b_1 \\\\
  \ell_{2,1} x_1 & + & \ell_{2,2} x_2 &   &        &   &                & = &    b_2 \\\\
          \vdots &   &         \vdots &   & \ddots &   &                &   & \vdots \\
  \ell_{m,1} x_1 & + & \ell_{m,2} x_2 & + & \dotsb & + & \ell_{m,m} x_m & = &    b_m \\
\end{matrix}
\end{equation*}
\\
The first equation does only involve $x_1$ and can be solved by simply inverting the relation. We can then substitute $x_1$ into the second equation and solve for $x_2$. By iterating this method, we can find the solution for each variable which takes the form
\\
\begin{equation*}
    x_i = \frac{b_i - \sum_{j=1}^{i-1}\ell_{i,j}\,x_j}{\ell_{i,i}} \;.
\end{equation*}
\\
Thus, given a linear triangular system with n equations, the algorithm requires $\mathcal{O}(n)$ steps to find the complete set of solutions. The backward substitution works as the forward substitution, but the iteration procedure is inverted. \textcolor{red}{Should I insert an explicit form for the solution in the backward propagation case?}

\subsection*{The LU factorisation}
It can be shown that if a matrix $\mb{V} \in \mathbb{R}^{n\times n}$ is nonsingular, then it is possible to implement an algorithm that computes an unit lower triangular matrix $\mb{L} \in \mathbb{R}^{n\times n}$ and an upper triangular matrix $\mb{U} \in  \mathbb{R}^{n\times n}$ such that 
\\
\begin{equation*}
    \mb{V} = \mb{L} \mb{U} 
\end{equation*}
\\
and $\det\left( \mb{V} \right) = u_{1,1} \, u_{2,2} \, \dots \, u_{n,n} $. For more details and a guided proof of this statement see Ref.\cite{GoluVanl96}. 

\section{Special Linear System and Cholesky factorization}
When dealing with numerical analysis it can be fruitful to exploit symmetries and properties whenever they are present. We shall examine the case in which $\mb{V}$ is both symmetric and positive definite, allowing to define the Cholesky decomposition.

\subsection*{Symmetry and the $\mb{L} \mb{D} \mb{L}^T$ decomposition.}
If $\mb{V}$ is a symmetric and admits a LU decomposition, $\mb{V} = \mb{L} \mb{U}$, then $\mb{L}$ and $\mb{U}$ are related. In particular, it can be proved (\textit{e.g.} see Ref.\cite{GoluVanl96}) that $\mb{U} = \mb{D} \mb{L}^T$, where $\mb{D} \in \mathbb{R}^{n \times n}$ is a diagonal matrix. Thus, the matrix $\mb{V}$ admits the following decomposition
\\
\begin{equation*}
    \mb{V} = \mb{L} \mb{D} \mb{L}^T \;,
\end{equation*}
\\
which is also unique. This factorization is necessary to introduce the Cholesky decomposition which, in addition to the symmetry assumption, exploits the positiveness of the matrix $\mb{V}$.

\subsection{Cholensy factorization}
A matrix $\mb{V}\in \mathbb{R}^{n \times n}$ is said to be positive definite if $\mb{x}^T \mb{V} \mb{x} > 0$ for all nonzero $\mb{x} \in \mathbb{R}^{n}$. If $\mb{V}$ is also symmetric, then there exists an unique lower triangular matrix $\mb{G} \in \mathbb{R}^{n \times n}$ with positive diagonal entries such that
\\
\begin{equation*}
    \mb{V} = \mb{G} \mb{G}^T \;.
\end{equation*}
\\
The matrix $\mb{G}$ is called \textit{Cholesky factor} and its entries can be computed recursively as follows
\\
\begin{align*}
    &G_{ii} = \sqrt{V_{ii} - \sum_{k=1}^{i-1} G_{ik}^2}\\\\
    & G_{ij} = \frac{1}{G_{jj}} \left[ \mb{V}_{ij} - \sum_{k=1}^{j-1} G_{ik} G_{jk}\right] \hspace{5mm} \textrm{for} \hspace{5mm} i>j\\\\
    &G_{ij} = 0 \hspace{5mm} \textrm{for} \hspace{5mm} i<j \; .
\end{align*}
\\
The Cholesky decomposition can be used to solve linear system described by a positive definite, symmetric matrix $\mb{V}$. Indeed, the slution of the original system $\mb{V} \cdot \mb{x} = \mb{b}$ is then found solving two distinct triangular systems by applying forward substitution
\\
\begin{equation*}
    \mb{G}\cdot \mb{y} = \mb{b} \; \; \rightarrow \; \;  \mb{G}^{T}\cdot \mb{x} = \mb{y} \; \; \Longrightarrow \; \; \mb{V} \cdot\mb{x} = \mb{G} \mb{G}^T \cdot \mb{x} = \mb{G} \cdot \mb{y} = \mb{b}\; .
\end{equation*}
For a proof of the previous statements and an extended discussion of the Cholesky decomposition, see Ref.\cite{GoluVanl96}.\par
Most of the tools we have discussed so far are implemented in common and useful libraries, such us the Gnu Scientific Library, which is also exploited in the MAP collaboration.

\chapter{Expectation value of the error function}
During the minimization procedure for the fluctuated data points, the error function converges to 2. The reason is that the error function does not correspond to the $\chi^2$ distribution, as we shall see.\par
Let us consider a data set $\mathcal{D}$ with $N_{dat}$ data points with a general covariance matrix $V$. Hence, the global $\chi^2$ is defined as 
\\
\begin{equation}
    \chi^2 = \frac{1}{N_{dat}}\sum_{i,j=1}^{N_{dat}} \left(t_i - d_i \right) \left( V^{-1} \right)_{ij} \left(t_j - d_j \right)\
\label{eq:app_chi2}
\end{equation}
\\
where $t_i = t(Q^2_i, x_i)$ denotes the fitted function (the neural network in our case) and $D_i$ is the (non-fluctuated) central value given by the experiment. Equation \eqref{eq:app_chi2} may be written in matricial notation 
\\
\begin{equation}
    \chi^2 = \frac{1}{N_{dat}} (\mb{t} - \mb{d})^{T} \, \mb{V}^{-1} \, (\mb{t} - \mb{d}) = \frac{1}{N_{dat}} \mb{x}^T \, \mb{V}^{-1} \, \mb{x}.
    \label{eq:app_chi2mat}
\end{equation}
\\
In this notation, $\mb{t}$ is a vector distributed according to a multivariate normal distribution $\mb{t} \sim \mathcal{N}(\mb{d},\mb{V})$, whereas $\mb{d} = (D_1,\; D_2,\; \dots,\;  D_{N_{dat}})$ is the vector containing the central values. In the second equality the residue has been substituted by a new random vector $\mb{x}$ that, by definition, is distributed according to $\mb{x} \sim \mathcal{N}(\mb{0},\; \mb{V})$. It is more convenient, and also more stylish, to use the Cholesky decomposition of the covariance matrix $V$. Being a symmetric and positive define matrix, $\mb{V}$ can be decomposed as
\\
\begin{equation}
    \mb{V} = \mb{L} \mb{L}^T \; ,
\label{eq:app_chodec}
\end{equation}
\\
where $\mb{L}$ is a lower triangular matrix. More detail on the Cholesky decomposition can be found in \textcolor{red}{appendix}. Then, it is straightforward to see that the $\chi^2$ can be written as
\\
\begin{equation}
    \chi^2 = \left| \mb{L}^{-1} \cdot \mb{x} \right|^2 \; .
\end{equation}
\\
We can define the vector $\boldsymbol{\chi} = \mb{L} \cdot \mb{x}$, which is the solution of the linear system
\\
\begin{equation}
    \mb{L} \cdot \boldsymbol{\chi} = \mb{x} \; ,
\end{equation}
\\
which can be solved by forward substitution, so that the $\chi^2$ can be efficiently computed as
\\
\begin{equation}
    \chi^2 = \norm{\boldsymbol{\chi}}^2 \; . 
\end{equation}
\\
With this procedure the computation of the inverse of the covariance matrix $\mb{V}$ is bypassed, leading to a significant simplification in terms of computational resources\footnote{\footnotesize{\textcolor{red}{This part concerning the Cholesky decomposition inserted in another chapter of the appendix or inside the main chapters.}}}.\par
By definition, the expectation value of the $\chi^2$ defined in \eqref{eq:app_chi2mat} is $\mathbb{E}[\chi^2] = 1$, since the residue is evaluated with respect to the central values. However, the expectation value is shifted when the residue is computed w.r.t. the fluctuated data points, which are generated with 
\\
\begin{equation}
    \mb{d}^{(k)} = \mb{d} + \mb{L} \cdot z^{(k)} 
\end{equation}
\\
where $z = (z_1, \; z_2, \; \dots \; z_{N_{dat}})$ is a vector distributed according to $\mb{z} \sim \mathcal{N}(\mb{0},\; I_{N_{dat}})$ such that
\\
\begin{equation}
    \left< z_i \right> = 0 \hspace{10mm} \left< z_i \, z_j \right> = \delta_i \, \delta_j \;.
\end{equation}
\\
The error function involved in the minimization procedure has the same definition of \eqref{eq:app_chi2}, but the quantities that it contains are different. Given the vector\footnote{\footnotesize{It is worth noting the vector $\mb{t}^{(k)}$ is a random vector distributed according to a multivariate normal distribution with mean value $\mb{d}$ and covariance matrix $\mb{V}$ for each replica.}} $\mb{t}^{(k)}$ that contains the points evaluated by the parametrization for the k-th artificial data set, the error function reads
\\
\begin{equation}
\begin{split}
    E^{(k)} & = \frac{1}{N_{dat}} \left(\mb{t}^{(k)} - \mb{d}^{(k)}\right)^T \, \mb{V}^{-1} \, \left( \mb{t}^{(k)} - \mb{d}^{k} \right) \\\\
    & = \frac{1}{N_{dat}} \left(\mb{t}^{(k)} - \mb{d} - \mb{L} \cdot \mb{z}^{(k)} \right)^T \, \mb{V}^{-1} \, \left(\mb{t}^{(k)} - \mb{d} - \mb{L} \cdot \mb{z}^{(k)} \right) \\\\
    & = \frac{1}{N_{dat}} \left( \mb{t}^{(k)} - \mb{d}\right)^T \, \mb{V}^{-1} \, \left( \mb{t}^{(k)} - \mb{d}\right) + \frac{1}{N_{dat}} \left( \mb{L} \cdot \mb{z}^{(k)} \right)^T \, \mb{V}^{-1} \left(  \mb{L} \cdot \mb{z}^{(k)} \right)\\\\
    & \hspace{5mm} - \frac{2}{N_{dat}} \left( \mb{t}^{(k)} - \mb{d}\right)^T \, \left(\mb{L}\mb{L}^{T} \right)^{-1} \, \left(\mb{L} \cdot \mb{z}^{(k)}\right)\\\\
    & = \left(\chi^2\right)^{(k)} + \frac{\left| \mb{z}^{(k)} \right|^2}{N_{dat}} - \frac{2}{N_{dat}} \left(\mb{w}^{(k)}\right)^T  \, \mb{z}^{(k)} \;,
\end{split}
\label{eq:app_errfun}
\end{equation}
\\
where $\mb{w}^{(k)} = \mb{L}^{-1} \cdot \left( \mb{t}^{(k)} - \mb{d} \right)$. The last equality shows that the error function is the sum of three different contributions. The first term is the $\chi^2$ of the k-the replica evaluated w.r.t. the central value. The second one is the norm of the random vector $\mb{z}^{(k)}$ normalized to the number of data points. The last one, more involved, is the product of two random variable normally distributed. By linearity, the expectation value of the error function over all the replicas is the sum of the expectation values of each term appearing in the last line of equation \eqref{eq:app_errfun}:
\\
\begin{equation}
\begin{split}
    \left< E^{(k)} \right> & = \frac{1}{N_{rep}} \sum_{k=1}^{N_{rep}} \left[ \left(\chi^2\right)^{(k)} + \frac{\left| \mb{z}^{(k)} \right|^2}{N_{dat}} - \frac{2}{N_{dat}} \left( \mb{L}^{-1} \cdot \mb{y}^{(k)}\right)^T  \, \mb{z}^{(k)}\right] \\\\
    & = \left<(\chi^2)^{(k)}\right>_{rep} + \frac{1}{N_{dat}} \left< \left| \mb{z}^{(k)} \right|^2 \right>_{rep} - \frac{2}{N_{dat}} \left<\left(\mb{w}^{(k)}\right)^T  \, \mb{z}^{(k)}\right>_{rep} \;.
\end{split}
\end{equation}
\\
The first term is 1 since it corresponds to the expectation value of a variable distributed according to a $\chi^2$ distribution. The second one is the sum of the expectation values of the squared elements contained in the vector $\mb{z}^{(k)}$. Remembering that $\mb{z} \sim \mathcal{N}(\mb{0},\, I_{N_{dat}}$ and that it contains $N_{dat}$ variables, the result is again 1. The last is zero, since it is the product of two normally distributed variables with null mean values. To see that, let us first understand how the vector $\mb{w}^{(k)}$ is distributed. To simplify the notation, I introduce a vector $\mb{y}^{(k)} = \mb{t}^{(k)} - \mb{d}$, which is distributed as $\mb{t}^{(k)}$ but with null mean values. Hence, we can write
\\
\begin{equation}
    \mb{w}^{(k)} = \mb{L}^{-1} \cdot \mb{y}^{(k)} \;.
\end{equation}
\\
The probability density function of $\mb{y}^{(k)}$ is a multivariate gaussian given by
\\
\begin{equation}
    f_{\mb{y}} = (2\pi)^{-N_{dat}/2} \; \left| \mb{V} \right|^{-1/2} \; \exp{-\frac{1}{2}\overline{\mb{y}}^{(k)} \; \mb{V}^{-1} \; \mb{y}^{(k)}},
\end{equation}
\\
where $\left| \mb{V} \right|$ denotes the determinant of the covariance matrix. Exploiting the Cholesky decomposition, the determinant can be written as 
\\
\begin{equation}
    \left| \mb{V} \right| = \left| \mb{L} \, \mb{L}^T \right| = \left| \mb{L} \right|^2 \;.
\end{equation}
\\
One can compute the probability density function of the random vector $\mb{w}^{(k)}$ as follows
\\
\begin{equation}
    f_{\mb{w}} = f_{\mb{y}} \left| \dv{\mb{y}}{\mb{w}} \right| = \left|\mb{L} \right| \, f_{\mb{y}} = (2\pi)^{-N_{dat}/2} \; \exp{-\frac{1}{2}\overline{\mb{w}}^{(k)} \; \mb{w}^{(k)}} \; .
\end{equation}
\\
As anticipated before, the product appearing in the last line of equation \eqref{eq:app_errfun} is distributed according to the convolution of two gaussian distributions both centered in zero. Since the mean value of convoluted gaussian is just the sum of the mean values of the parents gaussians, the expectation values of such a product is zero. We can conclude that the expectation value of the error function is $\left< E^{(k)} \right>_{rep}=2$, hence shifted by one respect to the centered global $\chi^2$ distribution defined in \eqref{eq:app_chi2}. 

\chapter{Lagrange interpolation}
\end{document}